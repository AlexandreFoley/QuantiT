\documentclass[15pt]{article}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.11}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{MA401}
\fancyhead[R]{Date: JAN/12/2018}
\fancyfoot[C]{\thepage}
\usepackage{graphicx}


\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\begin{document}
\title{Notes on libtorch}
\author{Alexandre Foley}

\maketitle

\tableofcontents
\section{Linear Algebra operations}
For tensors of real numbers, the number of decomposition and operations available is extensive. SVD,Eigenvalues,QR,LU and more are present. Of note: all matrix operation act on the last 2 indices of a tensor, the other index are treated as labeling independent matrices. For exemple:
\begin{verbatim}
    auto A = torch::rand({4,3,4,4});
    autop [E,U] = A.eig();
\end{verbatim}
will treat this as solving the eigenvalue problem of $4\times3=12$ size $4$ matrices. Therefor E is of dimension $\{4,3,4\}$ and U of dimension $\{ 4,3,4,4\}$.

The equivalent of dmrjulia's contract is torch::tensordot. It differ in two major way: order of arguments, and no overload to permute the output tensor.
\begin{verbatim}
    auto A = torch::rand({5,3,4});
    auto B = torch::rand({4,3,5});
    auto i_A = {1,2};
    auto i_B = {1,0};
    auto C = torch::tensordot(A,B,i_A,i_B); // 5x5 matrix
\end{verbatim}
The arguments are the 2 input tensor, and then the lists of index to contract in each tensor.

An arbitrary permutation can be done with torch::Tensor::permute, and tensors can be reshaped with torch::Tensor::reshape.
\subsection{Missing linear algebra kernels}

Some linear algebra kernels are not presently available within pytorch. Nothing fundamental stands in the way, they are simply not implemented.
The most notable of the missing algorithm are the following: LQ decomposition, rank-revealing QR and rank revealing LQ. LQ is simply the transpose of the QR algorithm.
Together, rank revealing LQ and rank revealing QR could replace the usage we make of SVD in general, for a somewhat cheaper numerical cost. (QR is a step in many SVD implementation)

Relevent documentation regarding how to preceed can be found in\\
pytorch/aten/src/ATen/native/cpu/README.md.
This file suggest that the yaml files might be a deprecated registration method. I am unsure.\\
pytorch/aten/src/ATen/native/README.md instruct to use the yaml for "native functions", and to use the registration macros for the content of the cpu folder.\\
https://pytorch.org/tutorials/advanced/dispatcher.html indicate that the macros is the mecanism of choice to extend pytorch from an external position.

The implementation of (not rank revealing) QR is a blueprint for the implementation of LQ.
This implementation contain the following:

\begin{enumerate}
    \item template on the type of number wrapper for the relevent lapack functions (xGEQRF, xORGQR and xUNGQR).
    \item wrapper for those preceding template that take care of the batching.
    \item Call the batched implementation and check lapack error codes. (\_qr\_helper\_cpu and \_qr\_helper\_cuda)
    \item Registering with the dispatcher.
    \item All the equivalent stuff for CUDA, etc..
\end{enumerate}
The registering to the dispatcher seems to be done using a generator that reads from native\_functions.yaml in pytorch/src/ATen/native/.
The relevent entry for QR are
\begin{verbatim}
- func: qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
use_c10_dispatcher: full
variants: method, function

- func: _qr_helper(Tensor self, bool some) -> (Tensor, Tensor)
use_c10_dispatcher: full
variants: function
dispatch:
CPU: _qr_helper_cpu
CUDA: _qr_helper_cuda
\end{verbatim}

The implemenation of qr\_backward is found in\\ pytorch/torch/csrc/autograd/FunctionsManual.cpp:$1843$ .
and for the derivative through QR, the registration yaml can be found in \\
pytorch/tools/autograd/derivatives.yaml:
\begin{verbatim}
- name: qr(Tensor self, bool some=True) -> (Tensor Q, Tensor R)
  self: qr_backward(grads, self, some, Q, R)
\end{verbatim}
\section{Tensor class structure}

The tensors of pytorch are build in a somewhat complicated way to offer a simple and uniform interface no matter what is the actual backend on which the computations are done.
This is accomplished by using the "pointer to implementation" idiom, "pimpl" for short. The torch::Tensor class is only a thin wrapper to a polymorphic pointer on the actual tensor, that tensor type depends on the actual backend but are all derived from the same base class. Those different tensor types therefor must offer the same interface.
Done this way, the end user gets all the advantages of the multiple backends with none of the additionnal difficulty associated with the differents type, not even polymorphic pointers.

While hiding a quantum tensor behind this interface is not impossible, any specific properties of quantum tensors would be innacessible. This means that constructing a quantum tensor would have to be done through a factory function, and from then on the user cannot do any direct manipulation to a property specific to quantum tensor.

\subsection{scantly documented methods and functions}
torch::Tensor have a slice(int direction,int begin,int end) method. this function gives a slice from the direction "direction" begining at begin and ending just before end.
For exemple
\begin{verbatim}
torch::Tensor a = torch::rand({5,5});
return a.slice(0,1,3); 
\end{verbatim}
will return a matrix containing a view on the rows [1,2].
The direction can be labeled in reversed order using negative numbers.


\subsection{Sparse tensors}
Torch implement a sparse tensor class. The format is similar to the implementation of the quantum tensor of DMRjulia: The non-zero coefficient and their indices are stored densely.

Sparse tensor, much like any potential quantum tensor implementation, require some differences from dense tensor in their interface. Emulating pytorch's implementers approach to these class for our quantum tensor is almost certainly the best way forward.

Also of note: augmenting this sparse tensor class for quantum tensor would closely emulate DMRjulia implementation.

\section{Complex numbers}
At the moment, complex numbers are badly supported by libtorch and not supported in the current release of pytorch.

Expression that contains both real numbers and complex numbers will cause type compatibility issues, explicit conversion to complex is currently necessary.

The number of supported linear algebra operations is limited. Currently SVD is supported on both CPU and GPU.
Eigen values is unavaible at the time of writing this.

The current status of Complex number support is tracked in \href{https://github.com/pytorch/pytorch/issues/33152}{issue 33152}




\end{document}

